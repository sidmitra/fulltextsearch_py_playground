{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Text Search\n",
    "\n",
    "This is a Python port of a golang implementation from [here](https://artem.krylysov.com/blog/2020/07/28/lets-build-a-full-text-search-engine/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The [latest dump](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract1.xml.gz) of abstract of English Wikipedia from dumps.wikimedia.org. The file size after decompression is 913 MB. The XML file contains over 600K documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file(zipped) already exists.\n",
      "Extracted enwiki-latest-abstract1.xml.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "\n",
    "CORPUS_URL = \"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract1.xml.gz\"\n",
    "zip_file = Path(CORPUS_URL.split(\"/\")[-1])\n",
    "xml_file = zip_file.name.replace(\".gz\", \"\")\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "        \n",
    "if not zip_file.exists():\n",
    "    print(\"Downloading data file(zipped).\")\n",
    "    download_url(CORPUS_URL, zip_file.name)\n",
    "else:\n",
    "    print(\"Data file(zipped) already exists.\")\n",
    "\n",
    "with gzip.open(zip_file, \"rb\") as f_in:\n",
    "    with open(xml_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Extracted {zip_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "class Benchmark:\n",
    "    \"\"\"\n",
    "    Helper context manager to profile time taken.\n",
    "    \"\"\"\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        t = timer() - self.start\n",
    "        print(f\"{self.msg} : {t:.2f} seconds\")\n",
    "        self.time = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docs : 25.38 seconds\n",
      "{'id': 0, 'title': 'Wikipedia: Anarchism', 'url': 'https://en.wikipedia.org/wiki/Anarchism', 'abstract': 'Anarchism is a political philosophy and movement that rejects all involuntary, coercive forms of hierarchy. It calls for the abolition of the state which it holds to be undesirable, unnecessary, and harmful.'}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as xml\n",
    "\n",
    "def load_documents(path):\n",
    "    tree = xml.parse(path)\n",
    "    root = tree.getroot()\n",
    "    docs = []\n",
    "    for index, xml_doc in enumerate(root):\n",
    "        doc = {\"id\": index}\n",
    "        for elm in xml_doc:\n",
    "            if elm.tag not in ['title', 'abstract', 'url']:\n",
    "                continue\n",
    "            doc[elm.tag] = elm.text or \"\"\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "with Benchmark(\"Loading docs\"):\n",
    "    docs = load_documents(xml_file)\n",
    "    \n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive searching cat : 0.21 seconds\n"
     ]
    }
   ],
   "source": [
    "def naive_search(docs, term):\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        if doc.get(\"abstract\") and term in doc[\"abstract\"]:\n",
    "            results.append(doc)\n",
    "    return results\n",
    "\n",
    "with Benchmark(\"Naive searching cat\"):\n",
    "    results = naive_search(docs, \"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import re\n",
    "\n",
    "def regex_search(docs, term):\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        re.compile()\n",
    "        results.append(doc)\n",
    "    return results\n",
    "#     re := regexp.MustCompile(`(?i)\\b` + term + `\\b`)\n",
    "#         if re.MatchString(doc.Text) {\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(\"\\w+\")\n",
    "    matches = pattern.finditer(text)\n",
    "    return [text[match.start():match.end()] for match in matches]\n",
    "    # TODO: Without finditer?\n",
    "    # return strings.FieldsFunc(text, func(r rune) bool {\n",
    "    #     // Split on any character that is not a letter or a number.\n",
    "    #     return !unicode.IsLetter(r) && !unicode.IsNumber(r)\n",
    "    # })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 from OEC\n",
    "# https://en.wikipedia.org/wiki/Most_common_words_in_English\n",
    "STOP_WORDS = set([\"a\", \"and\", \"be\", \"have\", \"i\", \"in\", \"of\", \"that\", \"the\", \"to\"])\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in STOP_WORDS]\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python\n",
    " - Porter Stemmer\n",
    " - Snowball Stemmers\n",
    " - What is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def reduce_to_stem(tokens):\n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the steps together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = lower(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = reduce_to_stem(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'donut': [1, 2], 'on': [1], 'glass': [1], 'plate': [1], 'onli': [1], 'is': [2]}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "class BaseIndex:\n",
    "    def __init__(self):\n",
    "        self.index: Dict[str, List[int]] = {}\n",
    "        \n",
    "    def add(self, doc: Dict[str, Union[int, str]]):\n",
    "        id_ = doc[\"id\"]\n",
    "        text = doc[\"text\"]\n",
    "            \n",
    "        for token in analyse(doc[\"text\"]):\n",
    "            ids = self.index.get(token, [])\n",
    "            if id_ not in ids:\n",
    "                # TODO: Ensure ascending order of Ids\n",
    "                ids.append(id_)\n",
    "                self.index[token] = ids\n",
    "            \n",
    "search_index = BaseIndex()\n",
    "search_index.add({\"id\": 1, \"text\": \"A donut on a glass plate. Only the donuts.\"})\n",
    "search_index.add({\"id\": 2, \"text\": \"donut is a donut\"})\n",
    "print(search_index.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexWithSearch(BaseIndex):\n",
    "    \"\"\"Extends the BaseIndex with search functionality.\"\"\"\n",
    "    \n",
    "    def search(self, text):\n",
    "        results = []\n",
    "        for token in analyse(text):\n",
    "            results.extend(self.index.get(token, []))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing abstract from XML : 37.08 seconds\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as xml\n",
    "\n",
    "def index_documents_from_xml(path, search_index, limit=None):\n",
    "    \"\"\"Load documents from the XML and add them to the index.\"\"\"\n",
    "    \n",
    "    tree = xml.parse(path)\n",
    "    root = tree.getroot()\n",
    "    for id_, xml_doc in enumerate(root):\n",
    "        # Adding first `limit` docs from corpus to the index\n",
    "        if limit:\n",
    "            if id_>0 and id_%100==0:\n",
    "                break\n",
    "                \n",
    "        doc = {\"id\": id_}\n",
    "        for elm in xml_doc:\n",
    "            # Ignore all other fields except abstract.\n",
    "            if elm.tag not in ['abstract']:\n",
    "                continue\n",
    "            abstract = elm.text or \"\"\n",
    "            search_index.add({\"id\": id_, \"text\": abstract})            \n",
    "    return\n",
    "\n",
    "# Creating an index with all the abstracts in the corpus\n",
    "search_index = IndexWithSearch()\n",
    "with Benchmark(\"Indexing abstract from XML\"):\n",
    "    index_documents_from_xml(xml_file, search_index, limit=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_index.search(\"solar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing abstract from XML : 26.47 seconds\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "\n",
    "class IndexWithBoolean(BaseIndex):\n",
    "    \"\"\"Extends the BaseIndex with search and boolean queries.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Change to set instead of list so we can do intersection\n",
    "        self.index: Dict[str, Set[int]] = {}\n",
    "        \n",
    "    def add(self, doc: Dict[str, Union[int, str]]):\n",
    "        id_ = doc[\"id\"]\n",
    "        text = doc[\"text\"]\n",
    "        for token in analyse(doc[\"text\"]):\n",
    "            ids = self.index.get(token, set())\n",
    "            if id_ not in ids:\n",
    "                ids.add(id_)\n",
    "                self.index[token] = ids\n",
    "                \n",
    "    def search(self, text):\n",
    "        results = set()\n",
    "        for token in analyse(text):\n",
    "            term_results = self.index.get(token, set())\n",
    "            if not results:\n",
    "                results = term_results\n",
    "            else:\n",
    "                results = results.intersection(term_results)\n",
    "        \n",
    "        return list(results)\n",
    "    \n",
    "    \n",
    "search_index = IndexWithBoolean()\n",
    "with Benchmark(\"Indexing abstract from XML\"):\n",
    "    index_documents_from_xml(xml_file, search_index, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing search : 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "with Benchmark(\"Indexing search\"):\n",
    "    search_index.search(\"is black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "- Extend boolean queries to support OR and NOT.\n",
    "- Store the index on disk:\n",
    "   - Rebuilding the index on every application restart may take a while.\n",
    "   - Large indexes may not fit in memory.\n",
    "\n",
    "- Experiment with memory and CPU-efficient data formats for storing sets of document IDs. Take a look at [Roaring Bitmaps](roaringbitmap.org).\n",
    "  See [blevesearch(golang)](https://blevesearch.com/).\n",
    "- Support indexing multiple document fields.\n",
    "- Sort results by relevance.\n",
    "\n",
    "- [TFIDF](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.8343)\n",
    "- BM25\n",
    "- Soundex for phonetic spelling errors\n",
    "- Stop word libs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
